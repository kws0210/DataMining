{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, time, gc\n",
    "import requests, shutil, random\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t\t (1225029, 3)\n",
      "Test:\t\t (117703, 2)\n",
      "Landmarks:\t 14951\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv('./data/all/train.csv')\n",
    "test = pd.read_csv('./data/all/test.csv')\n",
    "\n",
    "print('Train:\\t\\t', train.shape)\n",
    "print('Test:\\t\\t', test.shape)\n",
    "\n",
    "print('Landmarks:\\t', len(train['landmark_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cacf8152e2d2ae60</td>\n",
       "      <td>http://static.panoramio.com/photos/original/70...</td>\n",
       "      <td>4676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a58358a2afd3e4e</td>\n",
       "      <td>http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...</td>\n",
       "      <td>6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6b2bb500b6a38aa0</td>\n",
       "      <td>http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...</td>\n",
       "      <td>11284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b399f09dee9c3c67</td>\n",
       "      <td>https://lh3.googleusercontent.com/-LOW2cjAqubA...</td>\n",
       "      <td>8429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19ace29d77a5be66</td>\n",
       "      <td>https://lh5.googleusercontent.com/-tnmSXwQcWL8...</td>\n",
       "      <td>6231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                url  \\\n",
       "0  cacf8152e2d2ae60  http://static.panoramio.com/photos/original/70...   \n",
       "1  0a58358a2afd3e4e  http://lh6.ggpht.com/-igpT6wu0mIA/ROV8HnUuABI/...   \n",
       "2  6b2bb500b6a38aa0  http://lh6.ggpht.com/-vKr5G5MEusk/SR6r6SJi6mI/...   \n",
       "3  b399f09dee9c3c67  https://lh3.googleusercontent.com/-LOW2cjAqubA...   \n",
       "4  19ace29d77a5be66  https://lh5.googleusercontent.com/-tnmSXwQcWL8...   \n",
       "\n",
       "   landmark_id  \n",
       "0         4676  \n",
       "1         6651  \n",
       "2        11284  \n",
       "3         8429  \n",
       "4         6231  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Index for train, validation, and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* images >= 15: choose the first 10 as training set, the rest 2 as test images, and the last 3 as validation images\n",
    "* 12 < images < 15: keep the first 10 images as the training images, the rest 2 as test images, and the rest as validation images.\n",
    "* 10 < images <= 12: keep the first 10 images as the training images, the rest as test images.\n",
    "* 2 < images <= 10: choose the last one as test images, the rest as training images.\n",
    "* images <= 2: choose all as training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(path):\n",
    "    \"\"\" function to determine whether or not the given image is valid \"\"\"\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        if img.width < 256 or img.height < 256 or img.format != 'JPEG':\n",
    "            return False\n",
    "        _ = img.resize((256, 256))\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14951"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the unique ids\n",
    "unique_ids = sorted(train['landmark_id'].unique())\n",
    "len(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Process:        0\n",
      "===========================================================================\n",
      "Process:     3000\n",
      "===========================================================================\n",
      "Process:     6000\n",
      "===========================================================================\n",
      "Process:     9000\n",
      "===========================================================================\n",
      "Process:    12000\n",
      "=========================================================================="
     ]
    }
   ],
   "source": [
    "# fix random state\n",
    "np.random.seed(42)\n",
    "random.seed(29)\n",
    "\n",
    "# Split into training and test set\n",
    "train_idx = []\n",
    "val_idx = []\n",
    "test_idx = []\n",
    "\n",
    "for landmark_id in unique_ids:\n",
    "    # help information\n",
    "    if landmark_id % 3000 == 0:\n",
    "        print('\\nProcess: {:8d}'.format(landmark_id))\n",
    "    if landmark_id % 40 == 0:\n",
    "        print('=', end='')\n",
    "        \n",
    "    # get index corresponding to given landmark_id\n",
    "    index = list(train[train['landmark_id'] == landmark_id].index)\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    # check valid image numbers\n",
    "    valid_idx = []\n",
    "    for idx in index:\n",
    "        path = './data/all/train_images/' + str(idx) + '.jpg'\n",
    "        if valid(path):\n",
    "            valid_idx.append(idx)\n",
    "            \n",
    "        if len(valid_idx) >= 15:\n",
    "            break\n",
    "    \n",
    "    # split according to given rules\n",
    "    if len(valid_idx) >= 15:\n",
    "        train_idx = train_idx + valid_idx[:10]\n",
    "        test_idx = test_idx + valid_idx[10:12]\n",
    "        val_idx = val_idx + valid_idx[12:15]\n",
    "    elif len(valid_idx) > 12:\n",
    "        train_idx = train_idx + valid_idx[:10]\n",
    "        test_idx = test_idx + valid_idx[10:12]\n",
    "        val_idx = val_idx + valid_idx[12:]\n",
    "    elif len(valid_idx) > 10:\n",
    "        train_idx = train_idx + valid_idx[:10]\n",
    "        test_idx = test_idx + valid_idx[10:]\n",
    "    elif len(valid_idx) > 2:\n",
    "        train_idx = train_idx + valid_idx[:-1]\n",
    "        test_idx.append(valid_idx[-1])\n",
    "    elif len(valid_idx) > 0:\n",
    "        train_idx = train_idx + valid_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validation, and Test Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image information\n",
    "ids = train['id'].values\n",
    "urls = train['url'].values\n",
    "landmark_ids = train['landmark_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set\n",
    "train_image_id = []\n",
    "train_id = []\n",
    "train_url = []\n",
    "train_landmark_id = []\n",
    "\n",
    "for idx in train_idx:\n",
    "    from_path = './data/all/train_images/' + str(idx) + '.jpg'\n",
    "    to_path = './data/triplet/train/' + str(idx) + '.jpg'\n",
    "    img = io.imread(from_path)\n",
    "    resized = np.array(resize(img, (256, 256, 3), mode='reflect') * 255, dtype=np.uint8)\n",
    "    io.imsave(to_path, resized)\n",
    "    train_image_id.append(idx)\n",
    "    train_id.append(ids[idx])\n",
    "    train_url.append(urls[idx])\n",
    "    train_landmark_id.append(landmark_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk   \n",
    "train_df = pd.DataFrame({'image_id': train_image_id, 'id': train_id, \n",
    "                         'url': train_url, 'landmark_id': train_landmark_id})\n",
    "train_df.to_csv('./data/triplet/train.csv', index=False, \n",
    "                columns=['image_id', 'id', 'url', 'landmark_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split validation set\n",
    "val_image_id = []\n",
    "val_id = []\n",
    "val_url = []\n",
    "val_landmark_id = []\n",
    "\n",
    "for idx in val_idx:\n",
    "    from_path = './data/all/train_images/' + str(idx) + '.jpg'\n",
    "    to_path = './data/triplet/validation/' + str(idx) + '.jpg'\n",
    "    img = io.imread(from_path)\n",
    "    resized = np.array(resize(img, (256, 256, 3), mode='reflect') * 255, dtype=np.uint8)\n",
    "    io.imsave(to_path, resized)\n",
    "    val_image_id.append(idx)\n",
    "    val_id.append(ids[idx])\n",
    "    val_url.append(urls[idx])\n",
    "    val_landmark_id.append(landmark_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk   \n",
    "val_df = pd.DataFrame({'image_id': val_image_id, 'id': val_id, \n",
    "                       'url': val_url, 'landmark_id': val_landmark_id})\n",
    "val_df.to_csv('./data/triplet/validation.csv', index=False, \n",
    "              columns=['image_id', 'id', 'url', 'landmark_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test set\n",
    "test_image_id = []\n",
    "test_id = []\n",
    "test_url = []\n",
    "test_landmark_id = []\n",
    "\n",
    "for idx in test_idx:\n",
    "    from_path = './data/all/train_images/' + str(idx) + '.jpg'\n",
    "    to_path = './data/triplet/test/' + str(idx) + '.jpg'\n",
    "    img = io.imread(from_path)\n",
    "    resized = np.array(resize(img, (256, 256, 3), mode='reflect') * 255, dtype=np.uint8)\n",
    "    io.imsave(to_path, resized)\n",
    "    test_image_id.append(idx)\n",
    "    test_id.append(ids[idx])\n",
    "    test_url.append(urls[idx])\n",
    "    test_landmark_id.append(landmark_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk   \n",
    "test_df = pd.DataFrame({'image_id': test_image_id, 'id': test_id, \n",
    "                        'url': test_url, 'landmark_id': test_landmark_id})\n",
    "test_df.to_csv('./data/triplet/test.csv', index=False, \n",
    "               columns=['image_id', 'id', 'url', 'landmark_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t\t (113783, 4)\n",
      "Validation:\t (22255, 4)\n",
      "Test:\t\t (22391, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Train:\\t\\t', train_df.shape)\n",
    "print('Validation:\\t', val_df.shape)\n",
    "print('Test:\\t\\t', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
